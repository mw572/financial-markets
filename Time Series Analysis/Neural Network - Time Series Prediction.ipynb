{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Neural Network - Time Series Prediction\n",
    "\n",
    "####Based off shallow NNet architecture, inputs consist of data lagged 5,10,15 days with predictions consisting of a directional forecast of the index in 5 days time. \n",
    "\n",
    "The usage of weekly lagged data reduces issues with the fact that market [returns over the weekend](http://onlinelibrary.wiley.com/doi/10.1111/j.1468-5957.1988.tb00130.x/abstract) are not the same as intra week returns. The notebook is divided up into the following stages:\n",
    "\n",
    "1. Introduction and Setup\n",
    "1. Basic Prediction - Price & Volume Data\n",
    "1. Advanced Prediction - Technical Indicators\n",
    "1. Further Investigation - Fear & Greed Indicator Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Introduction and Setup\n",
    "\n",
    "###Get our Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#importing libraries and tools\n",
    "%pylab inline\n",
    "from yahoo_finance import Share\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import talib as tal\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn import cross_validation\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using S&P 500 data for this example, we are collecting data from March 2012 to January 2015. We are collecting a larger sample of data called techtraindata for later usage with indicators which need backfilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adj_Close': '2058.899902',\n",
      " 'Close': '2058.899902',\n",
      " 'Date': '2014-12-31',\n",
      " 'High': '2085.580078',\n",
      " 'Low': '2057.939941',\n",
      " 'Open': '2082.110107',\n",
      " 'Symbol': '%5eGSPC',\n",
      " 'Volume': '2606070000'}\n"
     ]
    }
   ],
   "source": [
    "#s&p 500\n",
    "spx = Share('^GSPC')\n",
    "\n",
    "#training data (note extended tech dataset for technical indicators)\n",
    "traindata = spx.get_historical('2012-03-01','2015-01-01') \n",
    "techtraindata = spx.get_historical('2011-12-01','2015-01-01')\n",
    "\n",
    "pprint(traindata[0]) #here is a sample of we have requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 775\n"
     ]
    }
   ],
   "source": [
    "print(len(traindata),len(techtraindata)) #this is the difference between our extended time series for the technical indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bringing a Neural Network to the party\n",
    "\n",
    "**This has one hidden layer with 6 neurons**. The number of hidden neurones was chosen by the calculation of **Sqrt(36 + 1) s 6**. It is trained with a Truncated Newton Constrained Algorithm. This code was gathered off stackoverflow (I believe) some time ago, I have been unable to match the code to an author after some searching. I do not take credit for its core construction, however I utilised its core architecture and made some changes off the back of [this](https://github.com/stephencwelch/Neural-Networks-Demystified) course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=6, opti_method='TNC', maxiter=500):\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.epsilon_init = epsilon_init\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation_func = self.sigmoid\n",
    "        self.activation_func_prime = self.sigmoid_prime\n",
    "        self.method = opti_method\n",
    "        self.maxiter = maxiter\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    def sumsqr(self, a):\n",
    "        return np.sum(a ** 2)\n",
    "    \n",
    "    def rand_init(self, l_in, l_out):\n",
    "        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init\n",
    "    \n",
    "    def pack_thetas(self, t1, t2):\n",
    "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
    "    \n",
    "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
    "        t1_start = 0\n",
    "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
    "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
    "        return t1, t2\n",
    "    \n",
    "    def _forward(self, X, t1, t2):\n",
    "        m = X.shape[0]\n",
    "        ones = None\n",
    "        if len(X.shape) == 1:\n",
    "            ones = np.array(1).reshape(1,)\n",
    "        else:\n",
    "            ones = np.ones(m).reshape(m,1)\n",
    "        \n",
    "        # Input layer\n",
    "        a1 = np.hstack((ones, X))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        z2 = np.dot(t1, a1.T)\n",
    "        a2 = self.activation_func(z2)\n",
    "        a2 = np.hstack((ones, a2.T))\n",
    "        \n",
    "        # Output layer\n",
    "        z3 = np.dot(t2, a2.T)\n",
    "        a3 = self.activation_func(z3)\n",
    "          \n",
    "        return a1, z2, a2, z3, a3\n",
    "    \n",
    "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
    "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        Y = np.eye(num_labels)[y]\n",
    "        \n",
    "        _, _, _, _, h = self._forward(X, t1, t2)\n",
    "        costPositive = -Y * np.log(h).T\n",
    "        costNegative = (1 - Y) * np.log(1 - h).T\n",
    "        cost = costPositive - costNegative\n",
    "        J = np.sum(cost) / m\n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            t1f = t1[:, 1:]\n",
    "            t2f = t2[:, 1:]\n",
    "            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))\n",
    "            J = J + reg\n",
    "        return J\n",
    "        \n",
    "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
    "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        t1f = t1[:, 1:]\n",
    "        t2f = t2[:, 1:]\n",
    "        Y = np.eye(num_labels)[y]\n",
    "        \n",
    "        Delta1, Delta2 = 0, 0\n",
    "        for i, row in enumerate(X):\n",
    "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
    "            \n",
    "            # Backprop\n",
    "            d3 = a3 - Y[i, :].T\n",
    "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)\n",
    "            \n",
    "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
    "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
    "            \n",
    "        Theta1_grad = (1 / m) * Delta1\n",
    "        Theta2_grad = (1 / m) * Delta2\n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
    "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
    "        \n",
    "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_features = X.shape[0]\n",
    "        input_layer_size = X.shape[1]\n",
    "        num_labels = len(set(y))\n",
    "        \n",
    "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
    "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
    "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
    "        \n",
    "        options = {'maxiter': self.maxiter}\n",
    "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
    "                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
    "        \n",
    "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Basic Prediction - Price & Volume Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our previous data import we are left with a dictionary of data in descending date order (as you could see from the previous pprint output).\n",
    "\n",
    "Below we go onto extract the high,low and close prices for the S&P 500 along with the volume for each day, we then normalise close and volume data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "highprice = np.array([row['High'] for row in techtraindata]).astype(np.float)\n",
    "\n",
    "lowprice = np.array([row['Low'] for row in techtraindata]).astype(np.float)\n",
    "\n",
    "close = np.array([row['Close'] for row in techtraindata]).astype(np.float)\n",
    "closeprices = pd.DataFrame(close / (close.max()*1.1))\n",
    "\n",
    "vol = np.array([row['Volume'] for row in techtraindata]).astype(np.float)\n",
    "volume = pd.DataFrame(vol / (vol.max()*1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is lagged in 5 day intervals and arrangd into an array. We call the unlagged column the forecast - this is so we can (if needed) see what we would be trying to predict from the other four columns. We are naming these columns t, lag5, lag10, lag15 however they are actually when considering the time series dates, lagged by 5 extra days. I deemed this to be a suitably simple method generating the data in a form that was easy to train the NNet with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755 714 775\n"
     ]
    }
   ],
   "source": [
    "#using unlagged value as forecast to make comparison against\n",
    "prices = pd.concat([closeprices, closeprices.shift(5), closeprices.shift(10), closeprices.shift(15), closeprices.shift(20)], axis=1)\n",
    "prices.columns = ['forcast','t','lag5','lag10','lag15']\n",
    "\n",
    "volumes = pd.concat([volume, volume.shift(5), volume.shift(10), volume.shift(15), volume.shift(20)], axis=1)\n",
    "volumes.columns = ['forecast', 't','lag5','lag10','lag15']\n",
    "\n",
    "prices = [[float(column) for column in row] for row in prices[prices.lag15.notnull()].values]\n",
    "volumes = [[float(column) for column in row] for row in volumes[volumes.lag15.notnull()].values]\n",
    "\n",
    "print(len(prices),len(traindata),len(techtraindata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have more data than our training data set, this is because we wanted to ensure when the data was lagged the minimum range (forecast vs max lag) had sufficent rows. In this case 755 is left after we lagged the original data (775) as 755>714 we are okay.\n",
    "\n",
    "Here we begin to put the data into the arrays that the neural network will recieve as an input and expected output to train from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694, 8) (694,)\n"
     ]
    }
   ],
   "source": [
    "X = [] ; y = []\n",
    "\n",
    "#iterating over our data\n",
    "for i in range(0,len(traindata)-20): #since we have lagged by 20 days at most\n",
    "    i = int(i) \n",
    "    val = 0\n",
    "    if prices[i+20][0]>prices[i+20][1]:\n",
    "        val = 1 #s&p 500 is higher the following week\n",
    "    else:\n",
    "        val = 0 #s&p 500 is lower the following week\n",
    "    \n",
    "    #combining our data into the training arrays\n",
    "    X.extend([[prices[i+20][1],prices[i+20][2],prices[i+20][3],prices[i+20][4],volumes[i+20][1],volumes[i+20][2], volumes[i+20][3], volumes[i+20][4]]])\n",
    "    y.extend([val]) #our predictions\n",
    "        \n",
    "\n",
    "X = np.array(X) ; y = np.array(y)\n",
    "\n",
    "print(X.shape, y.shape) #ensuring we have a match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Training the Model\n",
    "Before we begin to train the model, we partition our availble data into a training and test set so we can cross validate our predictions. We use a 90:10 split for training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 9:1 - train:test, initial epsilon=0.12, lambda=0\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.1)\n",
    "NN = Neural_Network()\n",
    "NN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61428571428571432"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, NN.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find the model accuracy of **61%** this is fairly low considering this is a fairly simple prediction requirement, however considering the lack of data that the model has it is not aweful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Prediction - Technical Indicators\n",
    "\n",
    "### Adding technical indicators (with lagged view)\n",
    "Please see [TA-LIB](https://github.com/mrjbq7/ta-lib) documentation for full details on the functions.\n",
    "\n",
    "####MACD Indicator Calculation\n",
    "\n",
    "**Moving Average Convergence / Divergence** is the difference betwen two exponentially smoothed moving averags of closing prices. Here we have used a fast period of 12, a slow period of 26, and a signal period of 9. The period choices were arbitrary as optimisation is needed to \"correctly\" choose such periods. These are supposedly the most commonly used settings. This is a suitable view to have as market participants will be looking at the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722 714 775\n"
     ]
    }
   ],
   "source": [
    "macd, macdsignal, macdhist = tal.MACD(np.fliplr([close])[0], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "macd = pd.DataFrame(macd);macdsignal = pd.DataFrame(macdsignal);macdhist = pd.DataFrame(macdhist)\n",
    "\n",
    "macd = macd[macd.notnull()].iloc[::-1];macdsignal = macdsignal[macdsignal.notnull()].iloc[::-1];macdhist = macdhist[macdhist.notnull()].iloc[::-1]\n",
    "macddata = pd.concat([macd, macd.shift(5), macd.shift(10), macd.shift(15), macd.shift(20),macdsignal, macdsignal.shift(5), macdsignal.shift(10), macdsignal.shift(15), macdsignal.shift(20), macdhist, macdhist.shift(5), macdhist.shift(10), macdhist.shift(15), macdhist.shift(20)], axis=1).dropna()\n",
    "\n",
    "macddata.columns = ['forcast','t','lag5','lag10','lag15','forcast','t','lag5','lag10','lag15','forcast','t','lag5','lag10','lag15']\n",
    "macddata = [[float(column) for column in row] for row in macddata.values]\n",
    "print(len(macddata),len(traindata),len(techtraindata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RSI Indicator Calculation\n",
    "\n",
    "**The Relative Strength Index** is an oscillator which gauges the relative strength of the security by considering up days and down days with a smoothing effect. Here a timeperiod of 14 has been chosen as this is the most popular setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 714 775\n"
     ]
    }
   ],
   "source": [
    "rsi = pd.DataFrame(tal.RSI(np.fliplr([close])[0], timeperiod=14))\n",
    "\n",
    "rsi = rsi[rsi.notnull()].iloc[::-1]\n",
    "rsidata = pd.concat([rsi, rsi.shift(5), rsi.shift(10), rsi.shift(15), rsi.shift(20)], axis=1).dropna()\n",
    "\n",
    "rsidata.columns = ['forcast','t','lag5','lag10','lag15']\n",
    "rsidata = [[float(column) for column in row] for row in rsidata.values]\n",
    "print(len(rsidata),len(traindata),len(techtraindata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADX Indicator Calculation\n",
    "\n",
    "**The Average Directional Index** quantifies trend strength based on moving averages, here we have set the most common setting as 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 714 775\n"
     ]
    }
   ],
   "source": [
    "adx = pd.DataFrame(tal.ADX(np.fliplr([highprice])[0], np.fliplr([lowprice])[0], np.fliplr([close])[0], timeperiod=14))\n",
    "\n",
    "adx = adx[adx.notnull()].iloc[::-1]\n",
    "adxdata = pd.concat([adx, adx.shift(5), adx.shift(10), adx.shift(15), adx.shift(20)], axis=1).dropna()\n",
    "\n",
    "adxdata.columns = ['forcast','t','lag5','lag10','lag15']\n",
    "adxdata = [[float(column) for column in row] for row in adxdata.values]\n",
    "print(len(adxdata),len(traindata),len(techtraindata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOCH Indicator Calculation\n",
    "\n",
    "**Stochastics** provides an oscillator that determines the close price in reference to recent trading range as an indicator of potential bullish and bearish trends ahead. Again popular settings are chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 714 775\n"
     ]
    }
   ],
   "source": [
    "slowk, slowd = tal.STOCH(np.fliplr([highprice])[0], np.fliplr([lowprice])[0], np.fliplr([close])[0], fastk_period=14, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "slowk = pd.DataFrame(slowk);slowd = pd.DataFrame(slowd)\n",
    "\n",
    "slowk = slowk[slowk.notnull()].iloc[::-1];slowd = slowd[slowd.notnull()].iloc[::-1]\n",
    "stochdata = pd.concat([slowk, slowk.shift(5), slowk.shift(10), slowk.shift(15), slowk.shift(20), slowd, slowd.shift(5), slowd.shift(10), slowd.shift(15), slowd.shift(20)], axis=1).dropna()\n",
    "\n",
    "stochdata.columns = ['forcast','t','lag5','lag10','lag15','forcast','t','lag5','lag10','lag15']\n",
    "stochdata = [[float(column) for column in row] for row in stochdata.values]\n",
    "print(len(stochdata),len(traindata),len(techtraindata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694, 36) (694,)\n"
     ]
    }
   ],
   "source": [
    "#put our data into an array ready for the neural network:\n",
    "\n",
    "X = [] #creating a list\n",
    "y = []\n",
    "\n",
    "for i in range(0,len(traindata)-20):\n",
    "    i = int(i)\n",
    "    val = 0\n",
    "    if prices[i+20][0]>prices[i+20][1]:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    \n",
    "    X.extend([[prices[i+20][1],prices[i+20][2],prices[i+20][3],prices[i+20][4],volumes[i+20][1],volumes[i+20][2], volumes[i+20][3], volumes[i+20][4], macddata[i+20][1],macddata[i+20][2],macddata[i+20][3],macddata[i+20][4], macddata[i+20][6],macddata[i+20][7],macddata[i+20][8],macddata[i+20][9],macddata[i+20][11],macddata[i+20][12],macddata[i+20][13],macddata[i+20][14],rsidata[i+20][1],rsidata[i+20][2],rsidata[i+20][3],rsidata[i+20][4],adxdata[i+20][1],adxdata[i+20][2],adxdata[i+20][3],adxdata[i+20][4],stochdata[i+20][1],stochdata[i+20][2],stochdata[i+20][3],stochdata[i+20][4],stochdata[i+20][6],stochdata[i+20][7],stochdata[i+20][8],stochdata[i+20][9]]])\n",
    "    y.extend([val])\n",
    "        \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91428571428571426"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.1)\n",
    "NN = Neural_Network()\n",
    "NN.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, NN.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a vastly improved accuracy of **91%** as compared to the model trained on only price and volume data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Further Investigation - Fear & Greed Indicator Implementation\n",
    "\n",
    "#### Adding a Fear & Greed Index\n",
    "\n",
    "We can look to other non price data such as the [Fear & Greed index](http://money.cnn.com/data/fear-and-greed/) that can give our model more predictive power using data that does not originate from internal price action.\n",
    "\n",
    "Incidently I have performed some Granger Causality analysis on this index, looking at its predictive power on the S&P500 index, it concluded that the index changes **did not** Granger-Cause the S&P 500 returns. However this must be revisited with more sophisticated techniques to remove seasonality and various factors affecting the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import our data \n",
    "with open('FGDATA.csv', 'r') as f:\n",
    "    data = [i.split(\",\") for i in f.read().split()]\n",
    "\n",
    "#we have data from 1st febuary 2014 to 1st jan 2015\n",
    "indexdata = np.array([row[1] for row in data]).astype(np.float)\n",
    "indexdata = np.fliplr([indexdata])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717 714 775\n"
     ]
    }
   ],
   "source": [
    "indexdata = pd.DataFrame(indexdata)\n",
    "index = pd.concat([indexdata, indexdata.shift(5), indexdata.shift(10), indexdata.shift(15), indexdata.shift(20)], axis=1)\n",
    "index.columns = ['forcast','t','lag5','lag10','lag15']\n",
    "index = [[float(column) for column in row] for row in index[index.lag15.notnull()].values]\n",
    "\n",
    "print(len(index),len(traindata),len(techtraindata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694, 12) (694,)\n"
     ]
    }
   ],
   "source": [
    "#put our data into an array ready for the neural network:\n",
    "\n",
    "X = [] #creating a list\n",
    "y = []\n",
    "\n",
    "for i in range(0,len(traindata)-20):\n",
    "    i = int(i)\n",
    "    val = 0\n",
    "    if prices[i+20][0]>prices[i+20][1]:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "        \n",
    "    X.extend([[prices[i+20][1],prices[i+20][2],prices[i+20][3],prices[i+20][4],volumes[i+20][1],volumes[i+20][2], volumes[i+20][3], volumes[i+20][4], index[i+20][1],index[i+20][2],index[i+20][3],index[i+20][4]]])\n",
    "    y.extend([val])\n",
    "        \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74285714285714288"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.1)\n",
    "NN = Neural_Network()\n",
    "NN.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, NN.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we see an inferior prediction accuracy of **74%** compared to the previous model, it is able to outperform the inital model with only price and volume but it does not have a superior predictive power to the model with technical indicators. It appears that despite the F&G index being a useful gauge of sentiment, it does not influence the directional move in the following week as much as technical indicators. This could be due to:\n",
    "\n",
    "* More people are probably looking to the common technical indicators rather than this bespoke index. \n",
    "* Market moves in my belief are self fufilling (unless large fundamental changes occur) people guage what others may make of data and act accordingly to cause their expectation unknowingly.\n",
    "* Diverse indicators are good at giving a overview of the market however specific changes on a weekly basis are less likely to have correlations to a general status indicator changes\n",
    "\n",
    "So overall we find that a neural network with one hidden layer with 6 neurons, trained with 36 inputs, consisting of lagged price data with indicators has **91% accuracy** at predicting the directional move of the S&P 500 in the following week.\n",
    "\n",
    "__I am looking to combine this model with an algorithm running multiple strategies to establish a directional bias in momentum trades.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
