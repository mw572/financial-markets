{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#WSJ Headline Deep Learning Model\n",
    "\n",
    "###Adapted/ inspired by the RaRe Machine Learning Blog Post\n",
    "\n",
<<<<<<< HEAD
    "####Raw Data was collected using some work from one of my other [projects](https://github.com/mw572/financial-markets/tree/master/Whitepaper%20Research/The%20financial%20press%20and%20stock%20markets%20in%20times%20of%20crisis)\n",
    "\n",
    "By Marcus Wiliamson 25/08/15"
=======
    "####Raw Data was collected using some work from one of my other [projects](https://github.com/mw572/financial-markets/tree/master/Whitepaper%20Research/The%20financial%20press%20and%20stock%20markets%20in%20times%20of%20crisis)"
    "\n",
    "By Marcus Williamson - 20/07/15
>>>>>>> origin/master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Preparing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import and setup\n",
    "from gensim import corpora, models, similarities, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(27258 unique tokens: ['legere', 'dogfight:', 'weightlifting', '$1.78', 'dé']...)\n"
     ]
    }
   ],
   "source": [
    "#Looking at our initial dictionary\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('rawdata/wsj1314.txt'))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(25702 unique tokens: ['legere', 'dogfight:', '$1.78', 'squeeze', 'reopening,']...)\n"
     ]
    }
   ],
   "source": [
    "#set stopwords and collect up id's of the stopwords\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "#remove stop & words occuring once\n",
    "dictionary.filter_tokens(stop_ids + once_ids) \n",
    "\n",
    "#removes gaps in id sequence after words that were removed\n",
    "dictionary.compactify() \n",
    "\n",
    "#see what we have left\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our Dictionary for later use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving\n",
    "dictionary.save('Dictionaries/wsj1314.dict') #storing dictionary for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bag of Words corpus  in efficient manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining class for creating vectors from corpus in memory efficient way (BOW)\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    #creating bow !!!    \n",
    "    def __iter__(self):\n",
    "        for line in open(self.fname): #pure python list\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_wsj = MyCorpus('rawdata/wsj1314.txt') #not loading into memory\n",
    "\n",
    "#saving vectorised corpus\n",
    "corpora.MmCorpus.serialize('Corpus/wsj1314.mm', corpus_wsj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading up what we have just created and saved to file\n",
    "dictionary = corpora.Dictionary.load('Dictionaries/wsj1314.dict')\n",
    "corpus = corpora.MmCorpus('Corpus/wsj1314.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Model creation & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create class to generate sentences from text files\n",
    "class MySentences(object):\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    def __iter__(self):\n",
    "         for line in open(self.fname): #pure python list\n",
    "                # assume there's one document per line, tokens separated by whitespace\n",
    "                yield line.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = MySentences('rawdata/wsj2014.txt') #using function to get our sentences\n",
    "\n",
    "#not going to print this all out..\n",
    "#for line in sentences:\n",
    "    #print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network with 40 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim #creating our neural model, 40 hidden layers, min word count 2\n",
    "model = gensim.models.Word2Vec(sentences, size=40, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add vocab\n",
    "sentences2 = MySentences('rawdata/wsj2013.txt') #using function to get our sentences\n",
    "model.build_vocab(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saving and loading the model\n",
    "model.save('Models/wsj1314.model')\n",
    "\n",
    "model = gensim.models.Word2Vec.load('Models/wsj1314.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205324"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train & trim model\n",
    "model.train(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('advance', 0.8725035190582275)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#implied relationships between words \n",
    "model.most_similar(positive=['rally',\"rise\"], negative=['selloff'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what doesnt match\n",
    "model.doesnt_match(\"citi barclays goldman google\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78224810903632902"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarities \n",
    "model.similarity('fomc','yellen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##More advanced model with phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_transformer = gensim.models.Phrases(sentences2)\n",
    "model2 = gensim.models.Word2Vec(bigram_transformer[sentences2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('healthcare', 0.7540712356567383)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying implied relationships - can now use phrases, limited dictionary\n",
    "model2.most_similar(positive=['goldman_sachs','ipo'], negative=['company'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80589303328207063"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarities \n",
    "model2.similarity('goldman_sachs','credit_suisse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('Models/wsj1314phrases.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating a TF-IDF (Term Frequency Inverse Document Frequency) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus('Corpus/wsj1314.mm')\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "corpus_tfidf = tfidf[corpus]# step 2 -- populate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Stacking TF-IDF with a LSI (Latent Semantic Indexing) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#like done previously\n",
    "dictionary = corpora.Dictionary.load('Dictionaries/wsj1314.dict')\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10) \n",
    "corpus_lsi = lsi[corpus_tfidf]  #bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with corpus_lsi which represents a bag of words corpus fed into a TF-IDF model which has been used to fold into an LSI model. The result allows us to examine topics existing within the data. Any reccuring themes and relationships should be visible below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.000*\"10-point.\" + 0.000*\"down\" + 0.000*\"cuts\" + 0.000*\"financial\" + 0.000*\"not\" + 0.000*\"currency\" + 0.000*\"hits\" + 0.000*\"out\" + 0.000*\"higher\" + 0.000*\"plan\"',\n",
       " '0.449*\"u.s.\" + 0.423*\"stocks\" + 0.324*\"asian\" + 0.281*\"briefing:\" + 0.280*\"morning\" + 0.202*\"on\" + 0.184*\"end\" + 0.156*\"lower\" + 0.142*\"higher\" + 0.114*\"shares\"',\n",
       " '0.383*\"on\" + -0.256*\"stocks\" + -0.255*\"briefing:\" + -0.255*\"morning\" + 0.237*\"profit\" + -0.231*\"asian\" + 0.169*\"as\" + 0.162*\"china\" + 0.159*\"oil\" + 0.155*\"prices\"',\n",
       " '-0.558*\"digest\" + -0.555*\"news\" + -0.414*\"watch:\" + -0.342*\"corporate\" + -0.148*\"briefing\" + -0.145*\"book:\" + -0.123*\"financial\" + 0.069*\"on\" + 0.054*\"profit\" + -0.050*\"world\"',\n",
       " '-0.617*\"profit\" + -0.255*\"rises\" + 0.229*\"new\" + -0.176*\"higher\" + -0.162*\"falls\" + 0.153*\"bank\" + 0.145*\"china\" + 0.145*\"bonds\" + 0.142*\"u.s.\" + 0.123*\"with\"',\n",
       " '0.307*\"bank\" + -0.307*\"oil\" + -0.305*\"prices\" + -0.269*\"u.s.\" + 0.226*\"profit\" + 0.226*\"new\" + -0.213*\"bonds\" + 0.199*\"china\" + 0.189*\"asian\" + -0.181*\"government\"',\n",
       " '0.401*\"u.s.\" + -0.313*\"shares\" + 0.300*\"bonds\" + -0.285*\"oil\" + -0.273*\"prices\" + -0.249*\"china\" + 0.244*\"government\" + 0.240*\"profit\" + -0.235*\"asian\" + -0.175*\"on\"',\n",
       " '-0.466*\"on\" + 0.374*\"oil\" + 0.306*\"prices\" + -0.294*\"shares\" + 0.228*\"as\" + -0.198*\"data\" + -0.159*\"bonds\" + -0.157*\"china\" + -0.148*\"dollar\" + 0.148*\"new\"',\n",
       " '-0.456*\"new\" + 0.333*\"china\" + 0.317*\"sales\" + -0.296*\"dollar\" + -0.243*\"on\" + -0.191*\"against\" + 0.152*\"rise\" + 0.151*\"bonds\" + -0.145*\"york\" + 0.133*\"government\"',\n",
       " '-0.400*\"bank\" + 0.393*\"new\" + -0.347*\"financial\" + -0.315*\"briefing\" + 0.304*\"corporate\" + -0.287*\"book:\" + 0.211*\"watch:\" + 0.195*\"sales\" + -0.195*\"central\" + 0.140*\"watch\"']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Saving our models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save our models\n",
    "lsi.save('Models/wsj1314.lsi')\n",
    "tfidf.save('Models/wsj1314.tfidf')\n",
    "\n",
    "#load our models\n",
    "lis = models.LsiModel.load('Models/wsj1314.lsi')\n",
    "tfidf = models.TfidfModel.load('Models/wsj1314.tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Without the corpus growing by about 50x this level of NLP is very sketchy.\n",
    "\n",
    "###It yields very little in terms of understanding implied links within the text. Headlines are notoriously sketchy in their conciseness, missing out key details and reliable phrasing as a tradeoff of brevity. If a years worth of articles, rather than headlines were used, it may have been a different outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

