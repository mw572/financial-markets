deparse("headlines$X",cat(deparse(i)),".headline")
headlines <- as.data.frame(datelists[i])
View(headlines)
colnames(headlines) <- c("day","headline");
View(headlines)
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
View(result)
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`))),counts[-(1:i),])
View(counts)
rm(headlines)
headlines <- as.data.frame(datelists[i])
colnames(headlines) <- c("day","headline");
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`))),counts[-(1:i),])
View(counts)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),stringsAsFactors=FALSE)
for(i in 1:10){
#generate our scores for a days worth of headlines
rm(headlines)
headlines <- as.data.frame(datelists[i])
colnames(headlines) <- c("day","headline");
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`))),counts[-(1:i),])
}
counts = counts[-1,]
View(counts)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),stringsAsFactors=FALSE)
for(i in 1:length(datelists)){
#generate our scores for a days worth of headlines
rm(headlines)
headlines <- as.data.frame(datelists[i])
colnames(headlines) <- c("day","headline");
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`))),counts[-(1:i),])
}
counts = counts[-1,]
View(counts)
View(counts)
View(counts)
write.table(counts, file="WSJ-2014-15-Market-counts.csv", quote = FALSE)
an <- c(afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1 | afinn_list$score==-4| afinn_list$score==-5], "second-rate", "moronic", "third-rate", "flawed", "juvenile", "boring", "distasteful", "ordinary", "disgusting", "senseless", "static", "brutal", "confused", "disappointing", "bloody", "silly", "tired", "predictable", "stupid", "uninteresting", "trite", "uneven", "outdated", "dreadful", "bland")
words
neg <- if(match(words, an)>=1){return 1}
if(match(words, an)>=1){neg=1}
match(words, an)
sum(match(words, an))
words = c("very", "bad", "ideas", "for", "2015")
words
sum(match(words, an))
match(words, an)
sum(!is.na(match(words,an)))
if(sum(!is.na(match(words, an)))>=1){neg=1}
neg
help("data.frame")
vals <- c(1:20)
vals
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
negarticles
View(result)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),neg=integer(),stringsAsFactors=FALSE)
vals <- c(1:20) #create a vector to compare counts of bad words against
for(i in 1:10){
#generate our scores for a days worth of headlines
rm(headlines)
headlines <- as.data.frame(datelists[i],row.names = c("day","headline"))
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`)),as.numeric(negarticles)),counts[-(1:i),])
}
counts = counts[-1,]
for(i in 1:10){
#generate our scores for a days worth of headlines
rm(headlines)
headlines <- as.data.frame(datelists[i],col.names = c("day","headline"))
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`)),as.numeric(negarticles)),counts[-(1:i),])
}
counts = counts[-1,]
View(counts)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),neg=integer(),stringsAsFactors=FALSE)
vals <- c(1:20) #create a vector to compare counts of bad words against
headlines <- as.data.frame()
#creating a loop to cycle through days
for(i in 1:10){
#generate our scores for a days worth of headlines
headlines <- as.data.frame(datelists[i],col.names = c("day","headline"))
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`)),as.numeric(negarticles)),counts[-(1:i),])
}
counts = counts[-1,]
View(counts)
headlines <- as.data.frame(datelists[i],col.names = c("day","headline"))
View(headlines)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),neg=integer(),stringsAsFactors=FALSE)
vals <- c(1:20) #create a vector to compare counts of bad words against
headlines <- as.data.frame()
#creating a loop to cycle through days
for(i in 1:10){
#generate our scores for a days worth of headlines
headlines <- as.data.frame(datelists[i])
colnames(headlines) = c("day","headline")
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`2`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`5`)),as.numeric(negarticles)),counts[-(1:i),])
}
counts = counts[-1,]
View(counts)
negarticles = count(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
result$`4`
View(counts)
View(result)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),neg=integer(),stringsAsFactors=FALSE)
vals <- c(1:20) #create a vector to compare counts of bad words against
headlines <- as.data.frame()
#creating a loop to cycle through days
for(i in 1:10){
#generate our scores for a days worth of headlines
headlines <- as.data.frame(datelists[i])
colnames(headlines) = c("day","headline")
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`5`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`2`)),as.numeric(negarticles)),counts[-(1:i),])
}
counts = counts[-1,]
View(counts)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),neg=integer(),stringsAsFactors=FALSE)
vals <- c(1:20) #create a vector to compare counts of bad words against
headlines <- as.data.frame()
#creating a loop to cycle through days
for(i in 1:length(datelists)){
#generate our scores for a days worth of headlines
headlines <- as.data.frame(datelists[i])
colnames(headlines) = c("day","headline")
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`5`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`2`)),as.numeric(negarticles)),counts[-(1:i),])
}
counts = counts[-1,]
#save table of data as backup
write.table(counts, file="WSJ-2014-15-Market-counts.csv", quote = FALSE)
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),neg=integer(),tot=integer(),stringsAsFactors=FALSE)
vals <- c(1:20) #create a vector to compare counts of bad words against
headlines <- as.data.frame()
#creating a loop to cycle through days
for(i in 1:length(datelists)){
#generate our scores for a days worth of headlines
headlines <- as.data.frame(datelists[i])
colnames(headlines) = c("day","headline")
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
totalarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`)|as.numeric(result$`3`)|as.numeric(result$`2`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`5`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`2`)),as.numeric(negarticles),as.numeric(totalarticles)),counts[-(1:i),])
}
counts = counts[-1,]
#save table of data as backup
write.table(counts, file="WSJ-2014-15-Market-counts.csv", quote = FALSE)
counts$tot
counts$tot[1]
badnewsindex <- as.data.frame(day=integer(),index=integer(),stringsAsFactors=FALSE)
for(i in 1:4){
#calculating w = c/l, (total/negative columns)
w = as.numeric(counts$tot[i])/as.numeric(counts$neg[i])
b = w*(1+as.numeric(counts$n[i])+as.numeric(counts$vn[i]))
badnewsindex <- rbind(badnewsindex[1:i,],c(i,b),badnewsindex[-(1:i),])
}
badnewsindex = badnewsindex[-1,]
badnewsindex <- data.frame(day=integer(),index=integer(),stringsAsFactors=FALSE)
for(i in 1:4){
#calculating w = c/l, (total/negative columns)
w = as.numeric(counts$tot[i])/as.numeric(counts$neg[i])
b = w*(1+as.numeric(counts$n[i])+as.numeric(counts$vn[i]))
badnewsindex <- rbind(badnewsindex[1:i,],c(i,b),badnewsindex[-(1:i),])
}
badnewsindex = badnewsindex[-1,]
View(badnewsindex)
for(i in 1:length(counts)){
#calculating w = c/l, (total/negative columns)
w = as.numeric(counts$tot[i])/as.numeric(counts$neg[i])
b = w*(1+as.numeric(counts$n[i])+as.numeric(counts$vn[i]))
badnewsindex <- rbind(badnewsindex[1:i,],c(i,b),badnewsindex[-(1:i),])
}
badnewsindex = badnewsindex[-1,]
View(badnewsindex)
length(counts)
nrow(counts)
rm(badnewsindex)
badnewsindex <- data.frame(day=integer(),index=integer(),stringsAsFactors=FALSE)
for(i in 1:nrow(counts)){
#calculating w = c/l, (total/negative columns)
w = as.numeric(counts$tot[i])/as.numeric(counts$neg[i])
b = w*(1+as.numeric(counts$n[i])+as.numeric(counts$vn[i]))
badnewsindex <- rbind(badnewsindex[1:i,],c(i,b),badnewsindex[-(1:i),])
}
badnewsindex = badnewsindex[-1,]
View(badnewsindex)
#save table of data as backup
write.table(counts, file="WSJ-2014-15-Market-index.csv", quote = FALSE)
#save table of data as backup
write.table(badnewsindex, file="WSJ-2014-15-Market-index.csv", quote = FALSE)
# ----------------------------------------------
# Script written by Marcus Williamson - 04/08/15
# ----------------------------------------------
# Part of Whitepaper research for title:
# "The financial press and stock markets in times of crisis"
# ----------------------------------------------
# Webscraping WSJ news
# ----------------------------------------------
#Cleanup the DB before we start
ls()
rm(list = ls())
# set working directory
setwd("/Users/marcuswilliamson/Documents/R/Whitepaper Research/The financial press and stock markets in times of crisis - Data Collection")
# load libraries
library(RCurl)
library(XML)
library(data.table)
library(plyr)
#how many pages of data do we have:
#SEARCHTERM IS MARKET
urlbase <- 'http://www.wsj.com/search/term.html?KEYWORDS=market&isAdvanced=true&min-date=2007/12/01&max-date=2009/07/01&page=1&daysback=4y&andor=AND&sort=date-desc&source=wsjarticle'
doc=htmlTreeParse(urlbase,useInternalNodes=TRUE)
ns = getNodeSet(doc,'//html/body/div[1]/div[2]/section[3]/div[1]/div[2]/div/div[2]/div[2]/menu/li[3]')
#this is our page count
count=as.numeric(gsub("[^0-9]","",sapply(ns,function(x) xmlValue(x))))
#part a + page no + part b give us the unique page link
urlbaseA <- "http://www.wsj.com/search/term.html?KEYWORDS=market&isAdvanced=true&min-date=2007/12/01&max-date=2009/07/01&page="
urlbaseB <- "&daysback=4y&andor=AND&sort=date-desc&source=wsjarticle"
#create our scraping function which takes url input and returns scraped output
scraper=function(urllink){
# get html page content
doc=htmlTreeParse(urllink,useInternalNodes=TRUE)
# getting node sets
#date of article
rawdate=getNodeSet(doc,"/html/body/div[1]/div[2]/section[3]/div[1]/div[2]/div/div/ul/li/div/div/div[2]/ul/li/time/text()")
#headline
headline=getNodeSet(doc,"/html/body/div[1]/div[2]/section[3]/div[1]/div[2]/div/div/ul/li/div/div/h3/a/text()")
#clean and put them in a table
rawdate=sapply(rawdate,function(x) xmlValue(x))
shortdate=lapply(rawdate,function(x) if(grepl("[.]",x)){as.Date(x,"%b. %d, %Y")} else{as.Date(x,"%b %d, %Y")}) #lapply to keep date format
shortdate=lapply(shortdate,function(x) as.character.Date(x)) #converting into string
datePublished=data.table(shortdate)
#put in table
headline=sapply(headline,function(x) xmlValue(x))
headline=data.table(headline)
# select the first price observation (Temp fix!)
#linePrice <- linePrice[1:1,]
scrapedfile <- cbind(datePublished, headline)
# put all the fields in a dataframe
return(scrapedfile)
}
#create location for our output
scrapedfile.l=as.list(rep(NA,count))
#for loop to do the scraping and placing in output
for(i in 1:count){
urlgen = paste(urlbaseA,i,urlbaseB,sep="",collapse = NULL)
scrapedfile.l[[i]]=scraper(urlgen)
print(paste(signif((i/count)*100,3),"%"))
}
#combining data outputs together
scrapedfile <- rbindlist(scrapedfile.l)
#Save our data to use later as backup
save(scrapedfile, file = "WSJ-08-Recession-Backup.Rdata") #rawbackup
#writing into table for csv output
output <- t(do.call(rbind,lapply(scrapedfile,matrix,ncol=nrow(scrapedfile),byrow=FALSE)))
#ensuring named correctly
colnames(output)[1:2] <- c("date","headline")
write.table(output, file="WSJ-08-Recession-Market.csv",sep="|", quote = FALSE)
bad_extended <- read.delim(file='badwordsextended.txt', header=FALSE, stringsAsFactors=FALSE)
View(bad_extended)
#import libraries
library(plyr)
library(stringr)
library(e1071)
#Cleanup the DB before we start
ls()
rm(list = ls())
setwd("~/Documents/R/Whitepaper Research/The financial press and stock markets in times of crisis - Data Collection")
#load up word polarity list and format it
afinn_list <- read.delim(file='AFINN-111.txt', header=FALSE, stringsAsFactors=FALSE)
names(afinn_list) <- c('word', 'score')
afinn_list$word <- tolower(afinn_list$word)
#load up our extended dictionary from recent recession wording
bad_extended <- read.delim(file='badwordsextended.txt', header=FALSE, stringsAsFactors=FALSE)
#categorize words as very n to very p and add some movie-specific words
vn <- c(afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4], bad_extended)
n <- c(afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1], "second-rate", "moronic", "third-rate", "flawed", "juvenile", "boring", "distasteful", "ordinary", "disgusting", "senseless", "static", "brutal", "confused", "disappointing", "bloody", "silly", "tired", "predictable", "stupid", "uninteresting", "trite", "uneven", "outdated", "dreadful", "bland")
p <- c(afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1], "first-rate", "insightful", "clever", "charming", "comical", "charismatic", "enjoyable", "absorbing", "sensitive", "intriguing", "powerful", "pleasant", "surprising", "thought-provoking", "imaginative", "unpretentious")
vp <- c(afinn_list$word[afinn_list$score==5 | afinn_list$score==4], "uproarious", "riveting", "fascinating", "dazzling", "legendary")
#load up our data - 2014-2015 "scrapedfile"
load("~/Documents/R/Whitepaper Research/The financial press and stock markets in times of crisis - Data Collection/WSJ-14-15-Backup.Rdata")
#function to calculate number of words in each category within a sentence
sentimentScore <- function(sentences, vn, n, p, vp){
final_scores <- matrix('', 0, 5)
scores <- laply(sentences, function(sentence, vn, n, p, vp){
initial_sentence <- sentence
#remove unnecessary characters and split up by word
sentence <- gsub('[[:punct:]]', '', sentence)
sentence <- gsub('[[:cntrl:]]', '', sentence)
sentence <- gsub('\\d+', '', sentence)
sentence <- tolower(sentence)
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vpm <- match(words, vp)
pm <- match(words, p)
vnm <- match(words, vn)
nm <- match(words, n)
#sum up number of words in each category
vpm <- sum(!is.na(vpm))
pm <- sum(!is.na(pm))
vnm <- sum(!is.na(vnm))
nm <- sum(!is.na(nm))
score <- c(vnm, nm, pm, vpm)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vn, n, p, vp)
return(scores)
}
#create dataframe for output
df <- data.frame(day=integer(),headline=character(),stringsAsFactors=FALSE)
a <- 1
#to iterate through this scoring function for each day
for(i in 1:nrow(scrapedfile)){
if(i==1){df <- rbind(df[1:1,],c(a,scrapedfile$headline[i]),df[-(1:1),])}
else if(as.character(scrapedfile$shortdate[i])==as.character(scrapedfile$shortdate[i-1])){df <- rbind(df[1:i,],c(a,scrapedfile$headline[i]),df[-(1:i),])}
else {
a = a + 1
df <- rbind(df[1:i,],c(a,scrapedfile$headline[i]),df[-(1:i),])
}
print(paste(signif((i/nrow(scrapedfile))*100,2),"%"))
}
#put in dataframe format and split by date so we have "daily headlines in lists" this may take a while to run
datelists <- split(df , f = df$day )
#create our counts dataframe to store totals by day
counts <- data.frame(day=integer(),vn=integer(),n=integer(),p=integer(),vp=integer(),neg=integer(),tot=integer(),stringsAsFactors=FALSE)
vals <- c(1:20) #create a vector to compare counts of bad words against
headlines <- as.data.frame()
#creating a loop to cycle through days
for(i in 1:length(datelists)){
#generate our scores for a days worth of headlines
headlines <- as.data.frame(datelists[i])
colnames(headlines) = c("day","headline")
#get our word counts
result <- as.data.frame(sentimentScore(headlines$headline, vn, n, p, vp),stringsAsFactors=FALSE)
negarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`),vals)))
totalarticles = sum(!is.na(match(as.numeric(result$`4`)|as.numeric(result$`5`)|as.numeric(result$`3`)|as.numeric(result$`2`),vals)))
#place them in the counts data frame
counts <- rbind(counts[1:i,],c(i,sum(as.numeric(result$`5`)),sum(as.numeric(result$`4`)),sum(as.numeric(result$`3`)),sum(as.numeric(result$`2`)),as.numeric(negarticles),as.numeric(totalarticles)),counts[-(1:i),])
}
counts = counts[-1,]
#save table of data as backup
write.table(counts, file="WSJ-2014-15-Market-counts-2.csv", quote = FALSE)
#THE BAD NEWS INDEX
#note: we are combining very bad & bad with very postive & positive
#formula: w = c/l (relative importance of article), b = w(t(n+1)) (bad news overall), we do not have t defined as the total available columns where news could appear
#
#looping through to create index for each day
badnewsindex <- data.frame(day=integer(),index=integer(),stringsAsFactors=FALSE)
for(i in 1:nrow(counts)){
#calculating w = c/l, (total/negative columns)
w = as.numeric(counts$tot[i])/as.numeric(counts$neg[i])
b = w*(1+as.numeric(counts$n[i])+as.numeric(counts$vn[i]))
badnewsindex <- rbind(badnewsindex[1:i,],c(i,b),badnewsindex[-(1:i),])
}
badnewsindex = badnewsindex[-1,]
#save table of data as backup
write.table(badnewsindex, file="WSJ-2014-15-Market-index-2.csv", quote = FALSE)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("caret")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
View(testing)
View(training)
View(predictors)
View(testing)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(mixtures$Superplasticizer,main="",xlab="")
View(mixtures)
hist(log10(mixtures$Superplasticizer),main="",xlab="")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
training['IL']
training['IL_']
View(training)
training$IL
training$IL
training['IL'*
training['IL']
training['IL']
training$type="IL"
training$type="IL"
training$type=="IL"
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
preProc <- preProcess(training$type=="IL",method="pca",pcaComp=2)
preProc <- preProcess(training[training$type=="IL"],method="pca",pcaComp=2)
preProc <- preProcess(training[[training$type=="IL"]],method="pca",pcaComp=2)
name <- grep("^IL", colnames(training), value = TRUE)
training[,"IL_11"]
training[,name]
preProc <- preProcess(training[,name], method = "pca", thresh = 0.9)
preProc$rotation
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
modelFit <- train($training[,name] ~ .,method="glm",preProcess="",data=training)
confusionMatrix(training[,name],predict(modelFit,testing))
modelFit <- train(training[,name] ~ .,method="glm",preProcess="",data=training)
confusionMatrix(training[,name],predict(modelFit,testing))
name <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[,name], method = "pca", thresh = 0.9)
trainPC <- predict(preProc,training[,names])
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
trainPC <- predict(preProc,training[,names])
trainPC <- predict(preProc,training)
View(training)
training = training[,name]
View(training)
training = adData[ inTrain,]
View(training)
name
name = c(name,"diagnosis")
name
training = training[,name]
View(training)
training = adData[ inTrain,]
name <- grep("^IL", colnames(training), value = TRUE)
name = c("diagnosis",name)
trainingnew = training[,name]
View(trainingnew)
preProc <- preProcess(trainingnew[,2:13], method = "pca", thresh = 0.9)
preProc <- preProcess(trainingnew[,2:13], method = "pca", thresh = 0.9)
#9 pcs
trainPC <- predict(preProc,training)
modelFit <- train(diagnosis ~ .,method="glm",data=trainPC)
preProc <- preProcess(trainingnew[,2:13], method = "pca", thresh = 0.9)
#9 pcs
trainPC <- predict(preProc,trainingnew)
modelFit <- train(diagnosis ~ .,method="glm",data=trainPC)
trainPC <- predict(preProc,trainingnew)
modelFit <- train(diagnosis ~ .,method="glm",preProcess="pca",data=trainingnew)
install.packages("e1071")
modelFit <- train(diagnosis ~ .,method="glm",preProcess="pca",data=trainingnew)
preProc <- preProcess(trainingnew[,2:13], method = "pca", thresh = 0.9)
#9 pcs
trainPC <- predict(preProc,trainingnew)
modelFit <- train(diagnosis ~ .,method="glm",data=trainPC)
modelFit <- train(diagnosis ~ .,method="glm",preProcess="pca",data=trainingnew)
predictions <- predict(modelFit, newdata = testing)
predictions <- predict(modelFit, newdata = testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
#second model without
modelFit <- train(diagnosis ~ ., method = "glm", data = trainingnew)
predictions <- predict(modelFit, newdata = testing)
C2 <- confusionMatrix(predictions, testing$diagnosis)
print(C2)
